{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\n",
      "Jupyter command `jupyter-notebook` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import albumentations as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "fix_all_seeds(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg: \n",
    "    BASE_PATH = cwd / '../Data'\n",
    "    CSV_PATH = BASE_PATH/'train.csv'\n",
    "    SEMI_PATH = BASE_PATH/'train_semi_supervised'\n",
    "    CELL_TYPE = ['shsy5y', 'astro', 'cort']\n",
    "    WIDTH, HEIGHT =  704, 520\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 0.001\n",
    "    EPOCHS=8\n",
    "    MASK_THRESHOLD = 0.5\n",
    "    KFOLD = 5\n",
    "    NUM_CLASSES = 2\n",
    "    BOX_DETECTIONS_PER_IMG = 539\n",
    "    DEVICE = 'cuda'\n",
    "    NORMALIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are slight redefinitions of torch.transformation classes\n",
    "# The difference is that they handle the target and the mask\n",
    "# Copied from Abishek, added new ones\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class VerticalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-2)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-2)\n",
    "        return image, target\n",
    "\n",
    "class HorizontalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, cfg.MEAN, cfg.STD)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [ToTensor()]\n",
    "    if cfg.NORMALIZE:\n",
    "        transforms.append(Normalize())\n",
    "    \n",
    "    # Data augmentation for train\n",
    "    if train: \n",
    "        transforms.append(HorizontalFlip(0.5))\n",
    "        transforms.append(VerticalFlip(0.5))\n",
    "\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None, resize=False):\n",
    "        self.transforms = transforms\n",
    "        self.df = df\n",
    "\n",
    "        self.should_resize = resize is not False\n",
    "        if self.should_resize:\n",
    "            self.height = int(cfg.HEIGHT * resize)\n",
    "            self.width = int(cfg.WIDTH * resize)\n",
    "        else:\n",
    "            self.height = cfg.HEIGHT\n",
    "            self.width = cfg.WIDTH\n",
    "\n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        temp_df = self.df.groupby('id')[['annotation', 'image_path']].agg(lambda x: list(x)).reset_index()\n",
    "        for index, row in temp_df.iterrows():\n",
    "            self.image_info[index] = {\n",
    "                'image_id': row['id'],\n",
    "                'image_path': row['image_path'][0],\n",
    "                'annotations': row[\"annotation\"]\n",
    "            }\n",
    "\n",
    "    def get_box(self, a_mask):\n",
    "        ''' Get the bounding box of a given mask '''\n",
    "        pos = np.where(a_mask)\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the image and the target'''\n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None, None\n",
    "        \n",
    "        if self.should_resize:\n",
    "            img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "\n",
    "        n_objects = len(info['annotations'])\n",
    "        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n",
    "        boxes = []\n",
    "\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = rle_decode(annotation, (cfg.HEIGHT, cfg.WIDTH))\n",
    "            a_mask = Image.fromarray(a_mask)\n",
    "\n",
    "            if self.should_resize:\n",
    "                a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "\n",
    "            a_mask = np.array(a_mask) > 0\n",
    "            masks[i, :, :] = a_mask\n",
    "\n",
    "            boxes.append(self.get_box(a_mask))\n",
    "\n",
    "        # dummy labels\n",
    "        labels = [1 for _ in range(n_objects)]\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n",
    "\n",
    "        # This is the required target for the Mask R-CNN\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.CSV_PATH) \n",
    "df['image_path'] = df['id'].apply(lambda x : (cfg.BASE_PATH/'train'/x).with_suffix('.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122   0   0   0   0]\n",
      " [  0 121   0   0   0]\n",
      " [  0   0 121   0   0]\n",
      " [  0   0   0 121   0]\n",
      " [  0   0   0   0 121]]\n"
     ]
    }
   ],
   "source": [
    "unique_id, idxes = np.unique(df['id'].values, return_index=True)\n",
    "cell_types = df.iloc[idxes]['cell_type'].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=cfg.KFOLD)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(unique_id,cell_types)):\n",
    "    df.loc[df['id'].isin(unique_id[test_index]),'fold'] = i \n",
    "    \n",
    "duplicat_list = []\n",
    "for i in range(cfg.KFOLD): \n",
    "    for j in range(cfg.KFOLD): \n",
    "        fdf = df[df['fold'] == i]\n",
    "        sdf = df[df['fold'] == j]\n",
    "        \n",
    "        duplicat_list.append(len(set(fdf['id']).intersection(sdf['id'])))\n",
    "\n",
    "duplite_metric = np.array(duplicat_list).reshape((cfg.KFOLD,cfg.KFOLD))\n",
    "print(duplite_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "train_df = df.query('fold != 4')\n",
    "valid_df = df.query('fold == 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CellDataset(train_df, resize=False, transforms=get_transform(train=True))\n",
    "image, annot = ds_train[0]\n",
    "dl_train = DataLoader(ds_train, batch_size=cfg.BATCH_SIZE, shuffle=True, \n",
    "                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "\n",
    "ds_val = CellDataset(valid_df, resize=False, transforms=get_transform(train=False))\n",
    "dl_val = DataLoader(ds_val, batch_size=cfg.BATCH_SIZE, shuffle=False, \n",
    "                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "\n",
    "print(annot['masks'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2)\n",
    "\n",
    "axs[0].imshow(image.permute((1,2,0)))\n",
    "axs[1].imshow(annot['masks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    if cfg.NORMALIZE:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, \n",
    "                                                                   box_detections_per_img=cfg.BOX_DETECTIONS_PER_IMG,\n",
    "                                                                   image_mean=cfg.MEAN, \n",
    "                                                                   image_std=cfg.STD)\n",
    "    else:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                  box_detections_per_img=cfg.BOX_DETECTIONS_PER_IMG)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, cfg.NUM_CLASSES)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, cfg.NUM_CLASSES)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the Mask R-CNN model\n",
    "# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n",
    "# We only care about MASKS\n",
    "model = get_model()\n",
    "model.to(cfg.DEVICE)\n",
    "\n",
    "# TODO: try removing this for\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=cfg.LR, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "n_batches, n_batches_val = len(dl_train), len(dl_val)\n",
    "validation_mask_losses = []\n",
    "\n",
    "for epoch in range(1, cfg.EPOCHS + 1):\n",
    "    print(f\"Starting epoch {epoch} of {cfg.EPOCHS}\")\n",
    "    \n",
    "    time_start = time.time()\n",
    "    loss_accum = 0.0\n",
    "    loss_mask_accum = 0.0\n",
    "    loss_classifier_accum = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n",
    "    \n",
    "        # Predict\n",
    "        images = list(image.to(cfg.DEVICE) for image in images)\n",
    "        targets = [{k: v.to(cfg.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_mask = loss_dict['loss_mask'].item()\n",
    "        loss_accum += loss.item()\n",
    "        loss_mask_accum += loss_mask\n",
    "        loss_classifier_accum  += loss_dict['loss_classifier'].item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}\")\n",
    "    \n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Train losses\n",
    "    train_loss = loss_accum / n_batches\n",
    "    train_loss_mask = loss_mask_accum / n_batches\n",
    "    train_loss_classifier = loss_classifier_accum / n_batches\n",
    "    \n",
    "    elapsed = time.time() - time_start\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n",
    "    prefix = f\"[Epoch {epoch:2d} / {cfg.EPOCHS:2d}]\"\n",
    "    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}\")\n",
    "    print(f\"{prefix} Train loss: {train_loss:7.3f}. [{elapsed:.0f} secs]\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss_accum = 0\n",
    "    val_loss_mask_accum = 0\n",
    "    val_loss_classifier_accum = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n",
    "            images = list(image.to(cfg.DEVICE) for image in images)\n",
    "            targets = [{k: v.to(cfg.DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n",
    "            val_loss_accum += val_batch_loss.item()\n",
    "            val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n",
    "            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n",
    "\n",
    "    # Validation losses\n",
    "    val_loss = val_loss_accum / n_batches_val\n",
    "    val_loss_mask = val_loss_mask_accum / n_batches_val\n",
    "    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n",
    "    elapsed = time.time() - time_start\n",
    "\n",
    "    validation_mask_losses.append(val_loss_mask)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.pth\")\n",
    "    prefix = f\"[Epoch {epoch:2d} / {cfg.EPOCHS:2d}]\"\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n",
    "    print(f\"{prefix} Val mask-only loss  : {val_loss_mask:7.3f}, classifier loss {val_loss_classifier:7.3f}\")\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n",
    "    print(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(labels, y_pred, verbose=0):\n",
    "    \"\"\"\n",
    "    Computes the IoU for instance labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        labels (np array): Labels.\n",
    "        y_pred (np array): predictions\n",
    "\n",
    "    Returns:\n",
    "        np array: IoU matrix, of size true_objects x pred_objects.\n",
    "    \"\"\"\n",
    "\n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of true objects: {}\".format(true_objects))\n",
    "        print(\"Number of predicted objects: {}\".format(pred_objects))\n",
    "\n",
    "    # Compute intersection between all objects\n",
    "    intersection = np.histogram2d(\n",
    "        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n",
    "    )[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins=true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "    intersection = intersection[1:, 1:] # exclude background\n",
    "    union = union[1:, 1:]\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "    \n",
    "    return iou  \n",
    "\n",
    "def precision_at(threshold, iou):\n",
    "    \"\"\"\n",
    "    Computes the precision at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Threshold.\n",
    "        iou (np array): IoU matrix.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of true positives,\n",
    "        int: Number of false positives,\n",
    "        int: Number of false negatives.\n",
    "    \"\"\"\n",
    "    matches = iou > threshold\n",
    "    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n",
    "    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "    tp, fp, fn = (\n",
    "        np.sum(true_positives),\n",
    "        np.sum(false_positives),\n",
    "        np.sum(false_negatives),\n",
    "    )\n",
    "    return tp, fp, fn\n",
    "\n",
    "def iou_map(truths, preds, verbose=0):\n",
    "    \"\"\"\n",
    "    Computes the metric for the competition.\n",
    "    Masks contain the segmented pixels where each object has one value associated,\n",
    "    and 0 is the background.\n",
    "\n",
    "    Args:\n",
    "        truths (list of masks): Ground truths.\n",
    "        preds (list of masks): Predictions.\n",
    "        verbose (int, optional): Whether to print infos. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        float: mAP.\n",
    "    \"\"\"\n",
    "    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        for iou in ious:\n",
    "            tp, fp, fn = precision_at(t, iou)\n",
    "            tps += tp\n",
    "            fps += fp\n",
    "            fns += fn\n",
    "\n",
    "        p = tps / (tps + fps + fns)\n",
    "        prec.append(p)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "\n",
    "    return np.mean(prec)\n",
    "\n",
    "\n",
    "def get_score(ds, mdl):\n",
    "    \"\"\"\n",
    "    Get average IOU mAP score for a dataset\n",
    "    \"\"\"\n",
    "    mdl.eval()\n",
    "    iouscore = 0\n",
    "    for i in tqdm(range(len(ds))):\n",
    "        img, targets = ds[i]\n",
    "        with torch.no_grad():\n",
    "            result = mdl([img.to(DEVICE)])[0]\n",
    "            \n",
    "        masks = combine_masks(targets['masks'], 0.5)\n",
    "        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n",
    "\n",
    "        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n",
    "        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n",
    "        iouscore += iou_map([masks],[pred_masks])\n",
    "    return iouscore / len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\n",
    "min_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\n",
    "\n",
    "def remove_overlapping_pixels(mask, other_masks):\n",
    "    for other_mask in other_masks:\n",
    "        if np.sum(np.logical_and(mask, other_mask)) > 0:\n",
    "            mask[np.logical_and(mask, other_mask)] = 0\n",
    "    return mask\n",
    "\n",
    "def get_filtered_masks(pred):\n",
    "    \"\"\"\n",
    "    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n",
    "    \"\"\"\n",
    "    use_masks = []   \n",
    "    for i, mask in enumerate(pred[\"masks\"]):\n",
    "\n",
    "        # Filter-out low-scoring results. Not tried yet.\n",
    "        scr = pred[\"scores\"][i].cpu().item()\n",
    "        label = pred[\"labels\"][i].cpu().item()\n",
    "        if scr > min_score_dict[label]:\n",
    "            mask = mask.cpu().numpy().squeeze()\n",
    "            # Keep only highly likely pixels\n",
    "            binary_mask = mask > mask_threshold_dict[label]\n",
    "            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n",
    "            use_masks.append(binary_mask)\n",
    "\n",
    "    return use_masks\n",
    "\n",
    "def combine_masks(masks, mask_threshold):\n",
    "    \"\"\"\n",
    "    combine masks into one image\n",
    "    \"\"\"\n",
    "    maskimg = np.zeros((cfg.HEIGHT, cfg.WIDTH))\n",
    "    # print(len(masks.shape), masks.shape)\n",
    "    for m, mask in enumerate(masks,1):\n",
    "        maskimg[mask>mask_threshold] = m\n",
    "    return maskimg\n",
    "# Plots: the image, The image + the ground truth mask, The image + the predicted mask\n",
    "\n",
    "def analyze_train_sample(model, ds_train, sample_index):\n",
    "    \n",
    "    img, targets = ds_train[sample_index]\n",
    "    #print(img.shape)\n",
    "    l = np.unique(targets[\"labels\"])\n",
    "    ig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n",
    "    ax[0].imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[0].set_title(f\"cell type {l}\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    masks = combine_masks(targets['masks'], 0.5)\n",
    "    #plt.imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[1].imshow(masks)\n",
    "    ax[1].set_title(f\"Ground truth, {len(targets['masks'])} cells\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model([img.to(cfg.DEVICE)])[0]\n",
    "    \n",
    "    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()\n",
    "    lstr = \"\"\n",
    "    for i in l.index:\n",
    "        lstr += f\"{l[i]}x{i} \"\n",
    "    #print(l, l.sort_values().index[-1])\n",
    "    #plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n",
    "    mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n",
    "    #print(mask_threshold)\n",
    "    pred_masks = combine_masks(get_filtered_masks(preds), mask_threshold)\n",
    "    ax[2].imshow(pred_masks)\n",
    "    ax[2].set_title(f\"Predictions, labels: {lstr}\")\n",
    "    ax[2].axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #print(masks.shape, pred_masks.shape)\n",
    "    score = iou_map([masks],[pred_masks])\n",
    "    print(\"Score:\", score)    \n",
    "    \n",
    "    \n",
    "# NOTE: It puts the model in eval mode!! Revert for re-training\n",
    "analyze_train_sample(model, ds_val, 20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
